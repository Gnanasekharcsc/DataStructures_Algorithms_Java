# Conflict with manager
## S [situation]
We were being flagged for dropping in availablity in few regions by thousand eyes report. They had a timeout for receiving response from our qos end point & if we didnt responded in time, they would count that a miss. On going through the logs we found that the app poll was being restarted & when this happens it takes a while for our cloud service to be intialized & be rady to respond. This correlated heavily with the misses thousand eyes was reporting to us. 
## T [task] + A[action]
 Meanwhile our sister team was running on beffier machines, we on the other hand were running on the tiniest complute vms. My manager has this opinin that we should upgrade to a slightly beffier machine in hopes that this improve our availablity. I was trying to get the figure on how much compute upgrade we would require inorded to serve optimally. When I captured & summarized the logs I foung that most of the times our vms are runnging with average cpu < 35%. Thus I had a strong feeling that upgrading to beefier machines was not the solution. On sharing this data with my manager she aggreed that our cpu utlisation was not very high, but she had the point that we might still benefit from upgrading to a beefire machines because we need extra capacity in case when we failover. Unfortunately we havent failed over in the last 30 days at that time so I couldnt lookup logs to support this theory. So I decided to dig depper into why the app pool are restarting, on readng the docs I found that there were logs file being written in one of the watched directory. When this log file was reaching the size it creates a new log file, & when this happens as we had our IIS by default confugured to watch this directory (FNC = File notification confgiruation  true) IIS would detect it as a new config & it would reload the configs by causing an app pool restart.
 ## R [result]
After figuring this out we saved ourselves 9600/year which would have costed us if we upgraded.



# failed to meet deadline
## S [situation] 
It was on my first few months on the job at Microsoft. There was this recommendation to run the payload dlls on the production nodes which are signed with the Azure certificate, in order to keep check that no malicious dlls were running on the nodes. Now it was time to move from recommendation to enforcement. This enforcement would mean they would not allow any unsigned dlls to be run in production upon detection. This meant we have to fix our dlls to be able to run in production in 3 weeks. The plan for the enforcement team was to run their enforcement in canary regions & then apply it to rest production regions. The colleague who was supposed to work on this left, & I was asked to step in & meet this dead line. I was new & like most other stuff I was yet to develop my grasp on this area. 
## T [task] + A[action]
I started going through the wiki to figure out how the signing was to be performed & which certificates were to be used for signing. My initial verification was to verify the dlls that are being generated in the retail output dir & verify if they are signed with the correct certificate or not. After verifying this I merged my changes & deployed our changes to the canary region. But we found out that we were still being flagged. Our initial guess was this violations could be coming from the staging slots which were active for us though they didnt served any traffic. We overwrite the staging slot in the canary & waited for their report to be kicked in. We were still being flagged & at this point we had to reach out to the team with our changes & verify that we did the correct thing or the wiki requires an update. This took our first week, the place we used to see these violations was a shared microsoft excel which was part of the violation incident. The response we got from the team was that sometimes this excel would show the old records, because the job required to update this doc sometimes fails & it could be because of this failure we are still seeing the violations, we were advisied to wait for another day. While we were in this back& forth some back we wasted few days & the deadline was upon us, while we were still being flagged for violations. Realising we would not be able to make it to deadline because if we had to make a change it would also had to deployed so we communicated a new deadline. It was a bit tough as we were the only team on which they were waiting on. On probing one of their team member in the office hours, we found that a team had faced a similar issue but it was not documented how did they resolved it. Plus we also a utitlity that we can run against our official build to get all the violations rather than depending on the excel doc to be updated. After running this agains our retail build we still didnt fount any violations, we deployed to canary & were still being flagged, after several hours of debugging I founf that depsite all the dlls that were generated were signed the cspkg package that would get uploaded to the nodes which was a zip file which contains all the dlls that will be execeting on the nodes was not containing the signed dlls. And because the cspkg was a zip it would not showup as a violation when run localy.
## R [result]
After finding this I changed the sequence of sigining & made it performs the signing before packaging the cspkg, with these changes we ran the utility after unzippping the genrated cspkg of the retail build & it didnt had the violations. We deplloyed this to canary & next day our canary violations were gone, later we deployed this change all over & unblocked the enforcement.

